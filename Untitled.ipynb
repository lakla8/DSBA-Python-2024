{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b265b13-5ff8-4b93-972a-fd4251e0dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "BASE_URL_LYRICS = \"https://soundtext.ru/{}/\"\n",
    "#BASE_URL_LYRICS = \"https://genius.com/{}-lyrics\"\n",
    "\n",
    "def get_lyrics_by_song(artist: str, track: str):\n",
    "    #с какого браузера пытаемся посмотреть\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(BASE_URL_LYRICS.format('-'.join(artist.replace(\"'\", \"\").split() + track.split())), headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        # exit() #close interpritator\n",
    "        return ''\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    soup = soup.find(\"div\", {\"data-lyrics-container\": \"true\"})\n",
    "    lyrics = [i.next_element.text.strip() for i in soup if i.next_element.text.strip()]\n",
    "    return '\\n'.join(lyrics)\n",
    "\n",
    "df = pd.read_csv('data/yandex_tracks_top100.csv')\n",
    "#eval - строчка - читает как выражение старается привести его к объектам x = 3 eval('x + 15')\n",
    "print(df['artist(s)'].apply(lambda x: len(eval(x))).value_counts())\n",
    "\n",
    "df.rename(columns={'artist(s)': 'artist', 'name': 'track'}, inplace=True)\n",
    "#mapper - {}\n",
    "print(unidecode.unidecode('царица'))\n",
    "for i, line in tqdm(df.iterrows()):\n",
    "    #i - series here\n",
    "    df.loc[i, 'lyrics'] = get_lyrics_by_song(line.artist, unidcode.unidecode(line.track))\n",
    "print(df.lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ca06005-9567-402f-ab32-9d5dfd743ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [в, время, экстракорпоральный, оплодотворение,...\n",
       "1      [в, настоящий, время, в, россия, около, 20, %,...\n",
       "2      [о, тот, или, иной, особенность, проведение, э...\n",
       "3      [вероятность, наступление, беременность, при, ...\n",
       "4      [вероятность, рождение, двойня, при, естествен...\n",
       "                             ...                        \n",
       "515    [оплодотворение, при, эко, осуществляться, в, ...\n",
       "516    [овариальный, резерв, ор, играть, важный, роль...\n",
       "517    [эффективность, экстракорпоральный, оплодотвор...\n",
       "518    [вспомогательный, репродуктивный, технология, ...\n",
       "519    [несмотря, на, тот, что, экстракорпоральный, о...\n",
       "Name: text, Length: 520, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natasha import (\n",
    "    Segmenter, #tokenization\n",
    "    MorphVocab, #какя часть речи род и тд\n",
    "\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger, #\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "#for token in doc.tokens:\n",
    "#>>>     token.lemmatize(morph_vocab) - in начальная форма - transfer all our words\n",
    "#doc.parse_syntax(syntax_parser) - subject / verb\n",
    "#>>> doc.tag_ner(ner_tagger) выделенеие именнованных сущностей - имя - локация - и что-то ещэ\n",
    "#stemming - only основа\n",
    "#for span in doc.spans:\n",
    "#>>>    span.normalize(morph_vocab) - future to infinitive - singal\n",
    "def prepare_words(words: str):\n",
    "    doc = Doc(words)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger) #на части речи\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    #doc.tokens #return tokens - by words and ,. etc\n",
    "    return [token.lemma for token in doc.tokens if token.pos != 'PUNCT']\n",
    "    \n",
    "df = pd.read_csv('data/Articles.csv')\n",
    "del df['Unnamed: 0.1']\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "subtitles = df.text.dropna().apply(prepare_words).reset_index(drop=True)#for dop index not emerge\n",
    "#apply return copy doesn't change col as 134 - just went out\n",
    "\n",
    "subtitles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "007ef0c0-59eb-4408-a510-0dde7489004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30937"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = {title: index for index, title in enumerate(set(subtitles.sum()))} #we ckleili lists\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7da06fdc-d151-447f-96e3-c3e1d0f69016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                             ...                        \n",
       "515    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "516    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "517    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "518    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "519    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: text, Length: 520, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_vector(words: str):\n",
    "    array = [0] * len(unique_words)\n",
    "    for word in words:\n",
    "        array[unique_words.get(word)] += 1\n",
    "    return np.array(array)\n",
    "\n",
    "def evklid(vec_1: np.array, vec_2: np.array):\n",
    "    sum_vecs = 0\n",
    "    diff = vec_1 - vec_2\n",
    "    diff = diff @ diff #we get 1-by-1\n",
    "    return np.sqrt(diff)\n",
    "#subtitles_array = subtitles.values\n",
    "subtitles_array = subtitles.apply(to_vector)\n",
    "\n",
    "subtitles_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "991600ca-4f5d-40f2-9d14-6658c0a2efaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 13.45362405,  5.56776436, ...,  4.47213595,\n",
       "        12.32882801,  5.74456265],\n",
       "       [ 0.        ,  0.        , 13.92838828, ..., 13.67479433,\n",
       "        17.74823935, 13.85640646],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  5.74456265,\n",
       "        13.22875656,  6.92820323],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        12.9614814 ,  6.08276253],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        , 12.68857754],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((subtitles_array.size, subtitles_array.size))\n",
    "for i in range(subtitles_array.size):\n",
    "    for j in range(i + 1, subtitles_array.size):\n",
    "        similarity_matrix[i, j] = evklid(subtitles_array[i], subtitles_array[j])\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2cfbbd5-a4ae-4ce9-b7a8-1f274aea3c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3      Вероятность наступления беременности при прове...\n",
       "52     Эндометрит и планирование беременности – тема,...\n",
       "54     Эмбрионы с неправильным количеством хромосом м...\n",
       "55     За несколько лет существования вспомогательных...\n",
       "57     Сегодня синдром поликистозных яичников (СПКЯ) ...\n",
       "                             ...                        \n",
       "510    «Меня зовут Алла Филаретовна, мне 64 года. Где...\n",
       "511    Безусловно, успех лечения сложных случаев рака...\n",
       "512    В течение последних 5 лет Чалов Владимир Ивано...\n",
       "514    ЭКО в естественном цикле подразумевает проведе...\n",
       "515    Оплодотворение при ЭКО осуществляется в день п...\n",
       "Name: text, Length: 188, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#класстаризацция\n",
    "#сколько слов мерность простравнсивг\n",
    "#https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(np.vstack(subtitles_array.values)) #n_init - form what to start/ random_state - fist arbitory point (saved), n_clusters - cnt of cnetress\n",
    "#fit - new organize predict - just predict but don'tchanhe\n",
    "#pd.Series(kmeans.labels_).value_counts()\n",
    "#np.where(kmeans.labels_==0) #all index of our texts\n",
    "df.text.iloc[np.where(kmeans.labels_==3)] #can do subtitile - to get jsut subtitle for texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3db87e5f-9a2f-4b41-acb8-e8821f4b58ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(subtitles_array.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b7461-1512-4d1f-bfe1-437c7eb14723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
